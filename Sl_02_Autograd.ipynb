{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWt2Q7sM4kxqUdpF0HYTHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susant146/PyTorch_Basics_CNNmodels/blob/main/Sl_02_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Autograd: Automatic Differentiation\n",
        "\n",
        "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "``torch.Tensor`` is the central class of the package. If you set its attribute\n",
        "``.requires_grad`` as ``True``, it **starts to track all operations on it**. When\n",
        "you finish your computation you can call ``.backward()`` and have **all the\n",
        "gradients computed automatically**. The gradient for this tensor will be\n",
        "accumulated into ``.grad`` attribute.\n",
        "\n",
        "To **stop a tensor from tracking history**, you can call ``.detach()`` to detach\n",
        "it from the computation history, and to prevent future computation from being\n",
        "tracked.\n",
        "\n",
        "To **prevent tracking history (and using memory)**, you can also wrap the code block\n",
        "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
        "model because the model may have trainable parameters with `requires_grad=True`,\n",
        "but for which we don't need the gradients."
      ],
      "metadata": {
        "id": "LuiRpHsnsun_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "KGYmt_ELsyLJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad = True)\n",
        "print(\"x: \", x)\n",
        "# Whenever we do operations with this tensor PyTorch will create a \"Computational graph\" and calculate gradient by back-tracking it.\n",
        "\n",
        "y = x+2\n",
        "print('The gard function\\t', y) #add backward\n",
        "\n",
        "z = y*y*2\n",
        "print('The gard function\\t', z) #mul backward\n",
        "z = z.mean()\n",
        "print('The gard function\\t', z) #mean backward\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f62fI6AuvDKP",
        "outputId": "95bfc54b-db5d-4289-e5ba-e5d30d76dd45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  tensor([ 0.3231, -0.5908, -0.2032], requires_grad=True)\n",
            "The gard function\t tensor([2.3231, 1.4092, 1.7968], grad_fn=<AddBackward0>)\n",
            "The gard function\t tensor([10.7933,  3.9719,  6.4567], grad_fn=<MulBackward0>)\n",
            "The gard function\t tensor(7.0739, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grad-Calculation & backward function**"
      ],
      "metadata": {
        "id": "ttZEm-oXxyIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()  # dz/dx\n",
        "# z = mean[2*(x+2)^2]\n",
        "print(x.grad)\n",
        "# Make x = torch.randn(3, requires_grad = False) and rerun again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7bF7LhWx2_A",
        "outputId": "93ddc4ac-e538-4596-bf0e-6c6e00f664cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.0974, 1.8790, 2.3957])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad = False\n",
        "x = torch.randn(3, requires_grad = False)\n",
        "print(\"x: \", x)\n",
        "# Whenever we do operations with this tensor PyTorch will create a \"Computational graph\" and calculate gradient by back-tracking it.\n",
        "\n",
        "y = x+2\n",
        "print('The gard function\\t', y) #add backward\n",
        "\n",
        "z = y*y*2\n",
        "print('The gard function\\t', z) #mul backward\n",
        "z = z.mean()\n",
        "print('The gard function\\t', z) #mean backward\n",
        "\n",
        "z.backward()  # Error here"
      ],
      "metadata": {
        "id": "NG5oQafozDop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain Rule**\n",
        "\n",
        "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
        "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\end{split}\n",
        "\n",
        "Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$, compute the product $v^{T}\\cdot J$. If $v$ happens to be the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$, that is, $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$, then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n",
        "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial y_{m}}\n",
        " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\end{split}\n",
        "\n",
        "(Note that $v^{T}\\cdot J$ gives a row vector which can be treated as a column vector by taking $J^{T}\\cdot v$.)\n",
        "\n",
        "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output."
      ],
      "metadata": {
        "id": "MgokUH0Cz24R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbbdshuP-mRz",
        "outputId": "bee85186-f47f-4027-f900-e2b24b210cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean() # Remove mean and then re-run\n",
        "               # Error = grad can be implicitly created only for dcalar outputs\n",
        "out.backward()\n",
        "print(x.grad)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "Let $out = \\frac{1}{4}\\sum_i z_i$,  \n",
        "$z_i = 3(x_i+2)^2$  \n",
        "and $z_i\\bigr\\rvert_{x_i=1} = 27$.  \n",
        "Therefore,  \n",
        "$\\frac{\\partial out}{\\partial x_i} = \\frac{1}{4}\\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4}.3.2(x_i+2) = \\frac{3}{2}(x_i+2)$,  \n",
        " hence  \n",
        "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$."
      ],
      "metadata": {
        "id": "eI5RH6mb0GDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Most of the case the final output is a scalar value\n",
        "# If not then we have to have a vector to complete the vector-Jacobian product\n",
        "x = torch.ones(3, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "# z = z.mean() # Remove mean and then re-run\n",
        "               # Error = grad can be implicitly created only for dcalar outputs\n",
        "z.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "wvDnSP4A2p9I",
        "outputId": "7670af72-0b6f-438b-b1b6-80b618187918"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f566c3c70c1a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# z = z.mean() # Remove mean and then re-run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;31m# Error = grad can be implicitly created only for dcalar outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mout_numel_is_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout_numel_is_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    199\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most of the case the final output is a scalar value\n",
        "# If not then we have to have a vector to complete the vector-Jacobian product\n",
        "x = torch.ones(3, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "# z = z.mean() # Remove mean and then re-run\n",
        "               # Error = grad can be implicitly created only for dcalar outputs\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
        "z.backward(v)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VXR1qQE3a_g",
        "outputId": "3b0d049a-c1dd-4a08-d6a5-0cd9bff826e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.8000e+00, 1.8000e+01, 1.8000e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preventing Gradient History**\n",
        "\n",
        "x.requires_grad_(False)\n",
        "\n",
        "x.detach()\n",
        "\n",
        "with torch.no_grad():"
      ],
      "metadata": {
        "id": "QUH5Tt2b37dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "x.requires_grad_(False)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrQ6t7Xb4AY7",
        "outputId": "656232f1-3d07-4b35-bc93-41bf436a334a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.9978, -1.0081, -0.3368], requires_grad=True)\n",
            "tensor([-0.9978, -1.0081, -0.3368])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x.detach()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ01O4UG6Fpz",
        "outputId": "7416952d-8726-4fd0-e0da-67ccdc659c6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.4535,  1.0971, -0.9018], requires_grad=True)\n",
            "tensor([-1.4535,  1.0971, -0.9018])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y = x+2\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY2sSo2g6PSM",
        "outputId": "66f9e474-6771-4754-f865-8c16a70e8f3f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2543, -0.8507,  0.4326], requires_grad=True)\n",
            "tensor([1.7457, 1.1493, 2.4326])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Example & Backprop With Autograd**"
      ],
      "metadata": {
        "id": "X5yKW0hO6zPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(f'Iteration {epoch}, Weight grads:, {weights.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBS0QflP65fw",
        "outputId": "7629acad-1957-4aa6-a64c-e845fb97b51e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Weight grads:, tensor([3., 3., 3., 3.])\n",
            "Iteration 1, Weight grads:, tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(f'Iteration {epoch}, Weight grads:, {weights.grad}')\n",
        "  # Very important\n",
        "  # Everytime the iteration runs for each epoch the weights are accumulated.\n",
        "  # We need to zero the grad to remove these accumularions. So that the next iteration can start afresh.\n",
        "\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpjJRyWJ8Rr5",
        "outputId": "d3d707d6-c3fc-42e0-fb5e-9444806d9a17"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Weight grads:, tensor([3., 3., 3., 3.])\n",
            "Iteration 1, Weight grads:, tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    }
  ]
}